<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>Background Audio</title>
</head>

<body>
    <script src="lib/vosk.js"></script>
    <script>
        const SAMPLE_RATE = 16000;

        let audioContext = null;
        let mediaStream = null;
        let workletNode = null;
        let recognizer = null;
        let model = null;
        let isPaused = false;
        let resumeCooldown = false; // Ignore detections right after resume
        let Vosk = window.Vosk;

        const log = (msg) => window.backgroundAudioApi.send('console-log', msg);
        const logError = (msg) => window.backgroundAudioApi.send('console-error', msg);

        async function initializeVosk(modelUrl) {
            try {
                // modelUrl is now an HTTP URL served by main process
                log('Loading Vosk model from: ' + modelUrl);

                if (!Vosk) {
                    // In case it wasn't valid immediately, try to get it again or wait
                    Vosk = window.Vosk;
                    if (!Vosk && window.exports && window.exports.Vosk) Vosk = window.exports.Vosk;
                }

                if (!Vosk) {
                    throw new Error('Vosk library not loaded properly');
                }

                // Vosk.createModel expects a URL.
                model = await Vosk.createModel(modelUrl);
                recognizer = new model.KaldiRecognizer(SAMPLE_RATE);

                recognizer.on('result', (event) => {
                    if (event.result && event.result.text) {
                        checkWakeWord(event.result.text);
                    }
                });

                recognizer.on('partialresult', (event) => {
                    if (event.result && event.result.partial) {
                        checkWakeWord(event.result.partial);
                    }
                });

                log('Vosk model loaded');
                return true;
            } catch (error) {
                logError('Vosk init failed: ' + error.message);
                return false;
            }
        }

        // Wake word patterns - THE ONLY PLACE these are used
        // Keep in sync with src/config/wake-word.config.js (reference only)
        const wakePatterns = [
            /hey\s*v[ei]n[aeiou]?s+[aeu]/i,   // "hey venesa/venessa" variations
            /hey\s*vanessa/i,                  // "hey vanessa"
            /hi\s*v[ei]n[aeiou]?s+[aeu]/i,    // "hi venesa" variations
            /hi\s*vanessa/i,                   // "hi vanessa"
            /\bv[ei]n[aeiou]?s+[aeu]\b/i,     // Just "venesa/venessa" standalone
            /\bvanessa\b/i,                    // Just "vanessa" standalone
        ];

        function checkWakeWord(text) {
            if (isPaused || resumeCooldown || !text) return;

            // Support both string and object formats (Vosk sometimes returns JSON)
            let cleanText = "";
            if (typeof text === 'string') {
                cleanText = text.toLowerCase().trim();
            } else if (text.text) {
                cleanText = text.text.toLowerCase().trim();
            } else {
                return;
            }

            for (const pattern of wakePatterns) {
                if (pattern.test(cleanText)) {
                    log('Wake word: ' + cleanText);
                    isPaused = true;
                    stopMic();
                    window.backgroundAudioApi.send('wake-word-detected', { wakeWord: 'hey_venessa', score: 1.0 });
                    return;
                }
            }
        }

        async function startMic() {
            try {
                if (audioContext) await audioContext.close().catch(() => { });

                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: { sampleRate: SAMPLE_RATE, channelCount: 1, echoCancellation: true, noiseSuppression: true }
                });

                audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
                await audioContext.audioWorklet.addModule('workers/audio.processor.js');

                const source = audioContext.createMediaStreamSource(mediaStream);
                workletNode = new AudioWorkletNode(audioContext, 'audio-capture-processor');

                workletNode.port.onmessage = (event) => {
                    if (event.data.type === 'audio' && recognizer && !isPaused) {
                        const float32 = event.data.buffer;
                        // acceptWaveformFloat expects Float32Array with values -1.0 to 1.0
                        recognizer.acceptWaveformFloat(float32, SAMPLE_RATE);
                    }
                };

                source.connect(workletNode);
                // Connect to a muted GainNode to keep audio graph active without audible playback (prevents feedback)
                const mutedGain = audioContext.createGain();
                mutedGain.gain.value = 0;
                workletNode.connect(mutedGain);
                mutedGain.connect(audioContext.destination);
                log('Mic started');
            } catch (error) {
                logError('Mic error: ' + error.message);
            }
        }

        function stopMic() {
            if (workletNode) { workletNode.disconnect(); workletNode = null; }
            if (mediaStream) { mediaStream.getTracks().forEach(t => t.stop()); mediaStream = null; }
            if (audioContext) { audioContext.close().catch(() => { }); audioContext = null; }
            window.backgroundAudioApi.send('mic-released');
            log('Mic stopped');
        }

        async function playAck() {
            try {
                const ctx = new AudioContext();
                if (ctx.state === 'suspended') {
                    await ctx.resume();
                }

                const osc = ctx.createOscillator();
                const gain = ctx.createGain();

                osc.frequency.setValueAtTime(880, ctx.currentTime);
                osc.frequency.setValueAtTime(1100, ctx.currentTime + 0.1);

                gain.gain.setValueAtTime(0.1, ctx.currentTime);
                gain.gain.exponentialRampToValueAtTime(0.001, ctx.currentTime + 0.2);

                osc.connect(gain);
                gain.connect(ctx.destination);

                osc.start();
                osc.stop(ctx.currentTime + 0.2);

                setTimeout(() => {
                    ctx.close().catch(e => logError('Error closing ctx: ' + e.message));
                }, 300);
            } catch (e) {
                logError('PlayAck failed: ' + e.message);
            }
        }

        window.backgroundAudioApi.receive('play-acknowledgment', () => playAck());
        window.backgroundAudioApi.receive('pause-detection', () => { isPaused = true; stopMic(); });
        window.backgroundAudioApi.receive('resume-detection', async () => {
            isPaused = false;
            resumeCooldown = true; // Ignore detections for a short period after resume
            if (recognizer) await startMic();
            // Clear cooldown after 1 second to avoid false triggers from leftover audio
            setTimeout(() => { resumeCooldown = false; log('Detection cooldown ended'); }, 1000);
        });

        window.backgroundAudioApi.receive('model-path', async (modelPath) => {
            if (await initializeVosk(modelPath)) {
                await startMic();
                window.backgroundAudioApi.send('background-audio-ready');
            }
        });

        window.backgroundAudioApi.send('get-model-paths');
        log('Background window loaded');
    </script>
</body>

</html>