<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>Background Audio</title>
</head>

<body>
    <script src="lib/vosk.js"></script>
    <script>
        const SAMPLE_RATE = 16000;

        let audioContext = null;
        let mediaStream = null;
        let workletNode = null;
        let recognizer = null;
        let model = null;
        let isPaused = false;
        let Vosk = window.Vosk;

        const log = (msg) => window.backgroundAudioApi.send('console-log', msg);
        const logError = (msg) => window.backgroundAudioApi.send('console-error', msg);

        async function initializeVosk(modelPath) {
            try {
                const fileUrl = 'file:///' + modelPath.replace(/\\/g, '/');
                log('Loading Vosk model from: ' + fileUrl);

                if (!Vosk) {
                    // In case it wasn't valid immediately, try to get it again or wait
                    Vosk = window.Vosk;
                    if (!Vosk && window.exports && window.exports.Vosk) Vosk = window.exports.Vosk;
                }

                if (!Vosk) {
                    throw new Error('Vosk library not loaded properly');
                }

                // Vosk.createModel expects a URL.
                model = await Vosk.createModel(fileUrl);
                recognizer = new model.KaldiRecognizer(SAMPLE_RATE);

                recognizer.on('result', (event) => {
                    if (event.result && event.result.text) {
                        checkWakeWord(event.result.text);
                    }
                });

                recognizer.on('partialresult', (event) => {
                    if (event.result && event.result.partial) {
                        checkWakeWord(event.result.partial);
                    }
                });

                log('Vosk model loaded');
                return true;
            } catch (error) {
                logError('Vosk init failed: ' + error.message);
                return false;
            }
        }

        function checkWakeWord(text) {
            if (isPaused || !text) return;

            // Support both string and object formats (Vosk sometimes returns JSON)
            let cleanText = "";
            if (typeof text === 'string') {
                cleanText = text.toLowerCase().trim();
            } else if (text.text) {
                cleanText = text.text.toLowerCase().trim();
            } else {
                return;
            }

            const patterns = [
                /hey\s*v[ei]n[aeiou]?s+[aeu]/i,
                /hey\s*vanessa/i,
                /hey\s*venesa/i,
                /hey\s*venus/i
            ];

            for (const pattern of patterns) {
                if (pattern.test(cleanText)) {
                    log('Wake word: ' + cleanText);
                    isPaused = true;
                    stopMic();
                    window.backgroundAudioApi.send('wake-word-detected', { wakeWord: 'hey_venessa', score: 1.0 });
                    return;
                }
            }
        }

        async function startMic() {
            try {
                if (audioContext) await audioContext.close().catch(() => { });

                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: { sampleRate: SAMPLE_RATE, channelCount: 1, echoCancellation: true, noiseSuppression: true }
                });

                audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
                await audioContext.audioWorklet.addModule('workers/audio.processor.js');

                const source = audioContext.createMediaStreamSource(mediaStream);
                workletNode = new AudioWorkletNode(audioContext, 'audio-capture-processor');

                workletNode.port.onmessage = (event) => {
                    if (event.data.type === 'audio' && recognizer && !isPaused) {
                        const float32 = event.data.buffer;
                        const int16 = new Int16Array(float32.length);
                        for (let i = 0; i < float32.length; i++) {
                            int16[i] = Math.max(-32768, Math.min(32767, Math.floor(float32[i] * 32768)));
                        }
                        recognizer.acceptWaveform(int16);
                    }
                };

                source.connect(workletNode);
                log('Mic started');
            } catch (error) {
                logError('Mic error: ' + error.message);
            }
        }

        function stopMic() {
            if (workletNode) { workletNode.disconnect(); workletNode = null; }
            if (mediaStream) { mediaStream.getTracks().forEach(t => t.stop()); mediaStream = null; }
            if (audioContext) { audioContext.close().catch(() => { }); audioContext = null; }
            window.backgroundAudioApi.send('mic-released');
            log('Mic stopped');
        }

        async function playAck() {
            try {
                const ctx = new AudioContext();
                const osc = ctx.createOscillator();
                const gain = ctx.createGain();
                osc.frequency.setValueAtTime(880, ctx.currentTime);
                osc.frequency.setValueAtTime(1100, ctx.currentTime + 0.1);
                gain.gain.setValueAtTime(0.1, ctx.currentTime);
                gain.gain.exponentialRampToValueAtTime(0.01, ctx.currentTime + 0.2);
                osc.connect(gain);
                gain.connect(ctx.destination);
                osc.start();
                osc.stop(ctx.currentTime + 0.2);
                setTimeout(() => ctx.close(), 300);
            } catch (e) { }
        }

        window.backgroundAudioApi.receive('play-acknowledgment', () => playAck());
        window.backgroundAudioApi.receive('pause-detection', () => { isPaused = true; stopMic(); });
        window.backgroundAudioApi.receive('resume-detection', async () => { isPaused = false; if (recognizer) await startMic(); });

        window.backgroundAudioApi.receive('model-path', async (modelPath) => {
            if (await initializeVosk(modelPath)) {
                await startMic();
                window.backgroundAudioApi.send('background-audio-ready');
            }
        });

        window.backgroundAudioApi.send('get-model-paths');
        log('Background window loaded');
    </script>
</body>

</html>